{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpihM2+SD7rIRTqfauYf/m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Waseem-687/Agentic-AI/blob/main/crewai_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lnm9Ms-P_aST"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U crewai crewai-tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xSouhh3SANqr",
        "outputId": "6cca152f-cfae-4942-e5d2-89b9d557b673"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyAb2cRv2HWqtpzexjH9gROk0wYKNcndy0Q'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"MODEL\"] = \"gemini/gemini-1.5-flash\"\n"
      ],
      "metadata": {
        "id": "jGlNIeNbBFs6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MODEL\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gVZHZ5PGB2AU",
        "outputId": "fc917e42-12db-4491-e000-13607de8ff98"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gemini/gemini-1.5-flash'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "G4P2rYKSB7Vo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!crewai create crew project3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlPRbQUWCE9v",
        "outputId": "2ef647b1-c6f8-4935-e8df-75a1dc9e1ce3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[1mCreating folder project3...\u001b[0m\n",
            "\u001b[36mSelect a provider to set up:\u001b[0m\n",
            "\u001b[36m1. openai\u001b[0m\n",
            "\u001b[36m2. anthropic\u001b[0m\n",
            "\u001b[36m3. gemini\u001b[0m\n",
            "\u001b[36m4. nvidia_nim\u001b[0m\n",
            "\u001b[36m5. groq\u001b[0m\n",
            "\u001b[36m6. ollama\u001b[0m\n",
            "\u001b[36m7. watson\u001b[0m\n",
            "\u001b[36m8. bedrock\u001b[0m\n",
            "\u001b[36m9. azure\u001b[0m\n",
            "\u001b[36m10. cerebras\u001b[0m\n",
            "\u001b[36m11. sambanova\u001b[0m\n",
            "\u001b[36m12. other\u001b[0m\n",
            "\u001b[36mq. Quit\u001b[0m\n",
            "Enter the number of your choice or 'q' to quit: 3\n",
            "\u001b[36mSelect a model to use for Gemini:\u001b[0m\n",
            "\u001b[36m1. gemini/gemini-1.5-flash\u001b[0m\n",
            "\u001b[36m2. gemini/gemini-1.5-pro\u001b[0m\n",
            "\u001b[36m3. gemini/gemini-gemma-2-9b-it\u001b[0m\n",
            "\u001b[36m4. gemini/gemini-gemma-2-27b-it\u001b[0m\n",
            "\u001b[36mq. Quit\u001b[0m\n",
            "Enter the number of your choice or 'q' to quit: 1\n",
            "Enter your GEMINI API key (press Enter to skip): AIzaSyB5YcbEZjsRBPIwBheJYxAbrvz6-MPTaEg\n",
            "\u001b[32mAPI keys and model saved to .env file\u001b[0m\n",
            "\u001b[32mSelected model: gemini/gemini-1.5-flash\u001b[0m\n",
            "\u001b[32m  - Created project3/.gitignore\u001b[0m\n",
            "\u001b[32m  - Created project3/pyproject.toml\u001b[0m\n",
            "\u001b[32m  - Created project3/README.md\u001b[0m\n",
            "\u001b[32m  - Created project3/knowledge/user_preference.txt\u001b[0m\n",
            "\u001b[32m  - Created project3/src/project3/__init__.py\u001b[0m\n",
            "\u001b[32m  - Created project3/src/project3/main.py\u001b[0m\n",
            "\u001b[32m  - Created project3/src/project3/crew.py\u001b[0m\n",
            "\u001b[32m  - Created project3/src/project3/tools/custom_tool.py\u001b[0m\n",
            "\u001b[32m  - Created project3/src/project3/tools/__init__.py\u001b[0m\n",
            "\u001b[32m  - Created project3/src/project3/config/agents.yaml\u001b[0m\n",
            "\u001b[32m  - Created project3/src/project3/config/tasks.yaml\u001b[0m\n",
            "\u001b[32m\u001b[1mCrew project3 created successfully!\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zt7-TkqVCy4S",
        "outputId": "0abb326b-ff92-4c77-cb49-e6193a1d4e0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd project3/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2Cd8O9YC2tQ",
        "outputId": "f7ab8bba-dc1d-46a8-a910-84cef9ef097c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/project3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lldBYW1pC_ZC",
        "outputId": "f658e262-5c93-40a4-de1a-2066f58203d8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knowledge  pyproject.toml  README.md  src  tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krECPZfUNWat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!crewai test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_Uz03WuDCd9",
        "outputId": "6fb1963d-7115-46cd-c16d-6dca291f45f0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the crew for 3 iterations with model gpt-4o-mini\n",
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mConduct a thorough research about AI LLMs Make sure you find any interesting and relevant information given the current year is 2025.\n",
            "\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "* **Multimodal LLMs are mainstream:**  2025 sees widespread adoption of LLMs capable of processing and generating multiple modalities, such as text, images, audio, and video.  This leads to more immersive and interactive applications, blurring the lines between different forms of media.  Examples include AI systems that can create storyboards from textual descriptions, generate realistic video game environments, or provide detailed image captions incorporating contextual audio cues.\n",
            "\n",
            "* **Personalized LLMs are prevalent:**  Advanced personalization techniques allow LLMs to adapt to individual users' styles, preferences, and knowledge levels, resulting in more tailored and effective interactions. This includes adaptive learning algorithms that modify the LLM's behavior based on user feedback and performance metrics, creating truly personalized AI assistants and educational tools.\n",
            "\n",
            "* **Enhanced reasoning and problem-solving capabilities:**  Significant advancements in reasoning and problem-solving have been made. LLMs are no longer limited to simple pattern recognition; they can handle complex logical deductions, solve mathematical problems, and even engage in strategic planning.  This is achieved through the integration of external knowledge bases, symbolic reasoning techniques, and improved architectural designs.\n",
            "\n",
            "* **Improved explainability and transparency:**  Efforts to make LLMs more transparent and interpretable have yielded notable results.  Techniques like attention visualization and model-agnostic explanations allow developers and users to understand the decision-making process of LLMs, fostering trust and addressing ethical concerns.\n",
            "\n",
            "* **Focus on reducing bias and promoting fairness:**  The AI community is actively working to mitigate biases in LLMs.  Advanced training techniques, bias detection algorithms, and data augmentation strategies are employed to ensure that LLMs are fair, equitable, and inclusive.  Emphasis is placed on diverse and representative datasets to reduce biases against particular groups.\n",
            "\n",
            "* **Increased efficiency and reduced computational costs:**  Research into efficient model architectures and training methods has led to LLMs that require significantly less computational power and energy.  This allows for deployment on smaller devices and reduces the environmental impact of AI.  Techniques like quantization, pruning, and knowledge distillation are commonplace.\n",
            "\n",
            "* **Robustness against adversarial attacks:**  LLMs are becoming more resistant to adversarial attacks, where malicious inputs aim to manipulate the model's output.  Advanced defense mechanisms and robust training methods are employed to enhance the resilience of LLMs to such attacks, ensuring reliable performance in diverse and potentially hostile environments.\n",
            "\n",
            "* **Integration with other AI technologies:** LLMs are increasingly integrated with other AI technologies such as computer vision, robotics, and reinforcement learning.  This creates powerful hybrid systems that combine the strengths of different AI approaches, leading to more sophisticated and versatile applications in various fields.\n",
            "\n",
            "* **Advanced control and safety mechanisms:**  Significant improvements in control and safety mechanisms are evident.  This includes techniques that allow users to guide and constrain the behavior of LLMs, ensuring that they operate within safe and ethical boundaries. This is crucial for high-stakes applications such as medical diagnosis and autonomous driving.\n",
            "\n",
            "* **Emergence of specialized LLMs:**  Instead of a single, general-purpose LLM, we are seeing a proliferation of specialized LLMs tailored for specific tasks or domains. This includes LLMs designed for scientific research, legal document analysis, or creative writing, resulting in more accurate and efficient solutions for niche applications.\u001b[00m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "ERROR:root:LiteLLM call failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[91m Error during LLM call: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[00m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py\", line 711, in completion\n",
            "    raise e\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py\", line 614, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py\", line 368, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "                  ^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/openai/_client.py\", line 110, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/main.py\", line 1692, in completion\n",
            "    raise e\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/main.py\", line 1665, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py\", line 721, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/project3/src/project3/main.py\", line 66, in test\n",
            "    Project3().crew().test(n_iterations=int(sys.argv[1]), openai_model_name=sys.argv[2], inputs=inputs)\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/crew.py\", line 1167, in test\n",
            "    test_crew.kickoff(inputs=inputs)\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/crew.py\", line 576, in kickoff\n",
            "    result = self._run_sequential_process()\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/crew.py\", line 683, in _run_sequential_process\n",
            "    return self._execute_tasks(self.tasks)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/crew.py\", line 781, in _execute_tasks\n",
            "    task_output = task.execute_sync(\n",
            "                  ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/task.py\", line 302, in execute_sync\n",
            "    return self._execute_core(agent, context, tools)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/task.py\", line 424, in _execute_core\n",
            "    self.callback(self.output)\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/utilities/evaluators/crew_evaluator_handler.py\", line 177, in evaluate\n",
            "    evaluation_result = evaluation_task.execute_sync()\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/task.py\", line 302, in execute_sync\n",
            "    return self._execute_core(agent, context, tools)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/task.py\", line 366, in _execute_core\n",
            "    result = agent.execute_task(\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/agent.py\", line 254, in execute_task\n",
            "    raise e\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/agent.py\", line 243, in execute_task\n",
            "    result = self.agent_executor.invoke(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 112, in invoke\n",
            "    raise e\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 102, in invoke\n",
            "    formatted_answer = self._invoke_loop()\n",
            "                       ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 160, in _invoke_loop\n",
            "    raise e\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 140, in _invoke_loop\n",
            "    answer = self._get_llm_response()\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 210, in _get_llm_response\n",
            "    raise e\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 201, in _get_llm_response\n",
            "    answer = self.llm.call(\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/crewai/llm.py\", line 291, in call\n",
            "    response = litellm.completion(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/utils.py\", line 1154, in wrapper\n",
            "    raise e\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/utils.py\", line 1032, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/main.py\", line 3068, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2201, in exception_type\n",
            "    raise e\n",
            "  File \"/content/project3/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 357, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/project3/.venv/bin/test\", line 10, in <module>\n",
            "    sys.exit(test())\n",
            "             ^^^^^^\n",
            "  File \"/content/project3/src/project3/main.py\", line 69, in test\n",
            "    raise Exception(f\"An error occurred while testing the crew: {e}\")\n",
            "Exception: An error occurred while testing the crew: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "An error occurred while testing the crew: Command '['uv', 'run', 'test', '3', 'gpt-4o-mini']' returned non-zero exit status 1.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}